\documentclass[12pt]{article}
\usepackage{amsmath, amssymb,amsthm,enumerate}
\usepackage{hyperref}
\usepackage{cite}
\usepackage[a4paper,bindingoffset=0.2in,%
left=0.8in,right=0.8in,top=1in,bottom=1in,%
footskip=.25in]{geometry}

\DeclareMathOperator*{\argmin}{argmin}
\newcommand{\ttt}[1]{\textbf{#1}}

\title{Advanced Methods Homework 1}
\date{\today}
\author{Bohao Tang}

\begin{document}

\maketitle

\section{Vector spaces and inner products}
\begin{enumerate}
   \item
   \begin{proof}
        Denote $\{\textbf{a}_1,\textbf{a}_2,\cdots,\textbf{a}_k\}$ be a basis of $W$.
        And we can expand them to a basis of $\mathbb{R}^n$ as $\{\textbf{a}_1,\textbf{a}_2,\cdots,\textbf{a}_k,\textbf{a}_{k+1},\cdots,\textbf{a}_n\}$.
        Then we do Gram-Schmidt orthogonalization to the basis and get 
        $\{\textbf{b}_1,\textbf{b}_2,\cdots,\textbf{b}_k,\textbf{b}_{k+1},\cdots,\textbf{b}_n\}$.
        It's easy to see that $\{\textbf{b}_1,\textbf{b}_2,\cdots,\textbf{b}_k\}$ become an othogonal basis of $W$
        (they are othogonal, hence linear independent, and $k$ is the dimension of $W$).
        Now we assert that $V = $ \textbf{span}$\{\textbf{b}_{k+1},\cdots,\textbf{b}_n\}$ will be $W^\bot$.
       
        First, for every vector $\textbf{x}$ in $W^\bot$, it has a representation $\textbf{x} = \sum_{i=1}^n \omega_i \textbf{b}_i$ for some $\omega_i \in \mathbb{R}^n$.
        Since $\textbf{x} \in W^\bot$, we have $\langle \textbf{x} , \textbf{b}_j \rangle = 0$, for all $j = 1,2,\cdots,k$.
        Therefore $\omega_j = 0$ for $1 \le j \le k$, which means $\textbf{x} \in V$ and $W^\bot \subset V$.
        
        In the other side, for every vector $\textbf{y} = \sum_{t=k+1}^n \gamma_t \textbf{b}_t \in V$, it's quite direct to see that $\langle \textbf{y} , \textbf{b}_j \rangle = 0$, for all $j = 1,2,\cdots,k$,
        hence $\langle \textbf{y} , \textbf{w} \rangle = 0$, for all $\textbf{w} \in W$.
        Therefore $\textbf{y} \in W^\bot$ and $W^\bot \subset V$.

        Now we get $W^\bot = V$, hence dim$(W^\bot) = n-k$. Also we can see that $W^\bot$ is unique. 
        If not, we can merge the two different $W^\bot$ and get a higher dimensional subspace $Z$ that is orthogonal to $W$.
        Then $Z \oplus W \subset \mathbb{R}^n$ but dim$(Z \oplus W) \ge n+1 > n$, a contradiction. 
    \end{proof}
    \item
    \begin{enumerate}[(a)]
        \item 
        \begin{proof}
            According to Cauchy inequality, $\left(\sum u_i v_i\right)^2 \le \sum u_i^2 \sum v_i^2$. It's equivalent here that $|\langle \textbf{u}, \textbf{v} \rangle| \le ||\textbf{u}|| ||\textbf{v}||$.
        \end{proof}
        \item
        \begin{proof}
            \begin{align}
                ||\textbf{u} + \textbf{v}||^2 + ||\textbf{u} - \textbf{v}||^2 &= (\textbf{u} + \textbf{v})'(\textbf{u} + \textbf{v}) + (\textbf{u} - \textbf{v})'(\textbf{u} - \textbf{v}) \\
                                                                              &= \textbf{u}'\textbf{u} + 2 \textbf{u}'\textbf{v} + \textbf{v}'\textbf{v} + \textbf{u}'\textbf{u} - 2 \textbf{u}'\textbf{v} + \textbf{v}'\textbf{v} \\
                                                                              &= 2 \textbf{u}'\textbf{u} + 2 \textbf{v}'\textbf{v} = 2 ||\textbf{u}||^2 + 2 ||\textbf{v}||^2
            \end{align}
        \end{proof}
    \end{enumerate}
    \item
    In the definition of projection in Lecture 2, $\textbf{y - $\Pi$(y|x)}$ should be orthogonal to $\textbf{x}$. So there is nothing need to be proved. 
    Maybe we should changed the definition here that (suppose $\textbf{x} \neq \textbf{0}$, or there is no meaningful projection)
    $$\Pi\textbf{(y|x)} \triangleq \argmin_{\textbf{u} \in \text{span}\{\textbf{x}\}} ||\textbf{y - u}||^2$$
    \begin{proof}
        (This proof contains the part to assert that projection above is well defined)
        Suppose $\textbf{u} = b \textbf{x}$, then $||\textbf{y} - \textbf{u}||^2 = \textbf{y}'\textbf{y} - 2 b \textbf{x}'\textbf{y} + b^2 \textbf{x}'\textbf{x}$.
        It's a quadratic function with highest coefficient $\textbf{x}'\textbf{x} > 0$, therefore have a unique minimizer.
        Differentiate it with $b$, let it be zero and we get the normal equation $\textbf{x}'\textbf{y} = b \textbf{x}'\textbf{x}$,
        which means (since $\Pi(\textbf{y|x}) = b \textbf{x}$) : $\langle \Pi(\textbf{y|x}), x \rangle = \langle \textbf{y}, x \rangle$ and therefore $\textbf{y} - \Pi(\textbf{y|x}) \  \bot \ \textbf{x}$.
    \end{proof}
\end{enumerate}

\section{Regression}

\begin{enumerate}
    \item 
    Slope $\hat{\beta}_{yx}$ of regressing \textbf{y} on \textbf{x} is $\hat{\rho}_{xy}\ \frac{\hat{\sigma}_y}{\hat{\sigma}_x}$, where $\hat{\rho}_{xy}$ is the sample correlation coefficient and $\hat{\sigma}_y, \hat{\sigma}_x$ are the sample standard deviation of $y$ and $x$.
    So, slope $\hat{\beta}_{xy}$ of \textbf{x} on \textbf{y} is $\hat{\rho}_{yx}\ \frac{\hat{\sigma}_x}{\hat{\sigma}_y}$.
    We have that $\hat{\rho}_{xy} = \hat{\rho}_{yx}$, so $\hat{\beta}_{xy} \hat{\beta}_{yx} = \hat{\rho}_{xy}^2$.
    \item
    \begin{proof}
        In the setting of mean only regression of \textbf{y}, we have $\hat{\mu} = \overline{\textbf{y}} = \sum_{i=1}^n y_i / n$.
        And the residue is $\textbf{r} = \textbf{y} - \overline{\textbf{y}} \textbf{J}_n$, therefore sum of residue is $\sum r_i = \sum_1^n y_i - n \cdot \sum_1^n y_i / n = 0$.
    \end{proof}
    \item
    \begin{proof}
        In the setting of mean only regression of \textbf{y} on \textbf{x}, we have $\hat{\beta} = \frac{\langle \textbf{y}, \textbf{x} \rangle}{\langle \textbf{x}, \textbf{x} \rangle}$.
        Therefore residue is $\textbf{r} = \textbf{y} - \frac{\langle \textbf{y}, \textbf{x} \rangle}{\langle \textbf{x}, \textbf{x} \rangle} \textbf{x}$, therefore $\langle \textbf{r}, \textbf{x} \rangle = \langle \textbf{y}, \textbf{x} \rangle - \frac{\langle \textbf{y}, \textbf{x} \rangle}{\langle \textbf{x}, \textbf{x} \rangle}\cdot\langle \textbf{x}, \textbf{x} \rangle = 0$, $\textbf{r} \bot \textbf{x}$.
    \end{proof}
    \item
    Notation as above, when $\textbf{y} \bot \textbf{x}$, we find that the residue $\textbf{r} = \textbf{y}$, which of course need not sum to zero.
    \item
    \begin{proof}
        In this setting of regressing \textbf{y} on \textbf{x}, we get the normal equation that:
        \begin{eqnarray}
            \textbf{J}_n'\textbf{y} &=& \hat{\beta}_0 \textbf{J}_n'\textbf{J}_n + \hat{\beta}_1 \textbf{J}_n'\textbf{x} \\
            \textbf{x}'\textbf{y} &=& \hat{\beta}_0 \textbf{x}'\textbf{J}_n + \hat{\beta}_1 \textbf{x}'\textbf{x}
        \end{eqnarray} 
    and also the residue $\textbf{r} = \textbf{y} - \hat{\beta}_0 \textbf{J}_n - \hat{\beta}_1 \textbf{x}$.
    Then: 
    $$\langle \textbf{r}, \textbf{J}_n \rangle = \textbf{J}_n'\textbf{y} - \hat{\beta}_0 \textbf{J}_n'\textbf{J}_n + \hat{\beta}_1 \textbf{J}_n'\textbf{x} = 0$$
    and
    $$\langle \textbf{r}, \textbf{x} \rangle = \textbf{x}'\textbf{y} - \hat{\beta}_0 \textbf{x}'\textbf{J}_n + \hat{\beta}_1 \textbf{x}'\textbf{x} = 0$$
    So we get the residue $\textbf{r} \bot \textbf{J}_n$ and $\textbf{r} \bot \textbf{x}$.
    \end{proof}
\end{enumerate}

\section{Least squares}

\begin{enumerate}
    \item
    \begin{proof}
        Given $\textbf{H}^2 = \textbf{H}$, then $\textbf{(I - H)}^2 = \textbf{I}^2 - 2 \textbf{H} + \textbf{H}^2 = \textbf{I} - 2 \textbf{H} + \textbf{H} = \textbf{I - H}$.
    \end{proof}
    \item
    \item
    \begin{proof}
        Suppose the singular value decomposition of $\ttt{X}$ is $\ttt{X} = \ttt{U} \ttt{S} \ttt{V}$, where 
    \end{proof}
    \item
\end{enumerate}

\end{document}